---
# IMPORTANT: Change settings here, but DO NOT change the spacing.
# Remove comments and add values where applicable.
# The descriptions below should be self-explanatory

title: "Volatility Spillovers from US to SA Markets"
#subtitle: "This will appear as Right Header"

documentclass: "elsarticle"

# --------- Thesis title (Optional - set to FALSE by default).
# You can move the details below around as you please.
Thesis_FP: FALSE
# Entry1: "An unbelievable study with a title spanning multiple lines."
# Entry2: "\\textbf{Some Guy}" # textbf for bold
# Entry3: "A thesis submitted toward the degree of Doctor of Philosophy"
# Uni_Logo: Tex/Logo.png # Place a logo in the indicated location (from your root, e.g. defaults to ~/Tex/Logo.png) and uncomment this line. Leave uncommented for no image
# Logo_width: 0.3 # If using a logo - use this to set width (size) of image
# Entry4: "Under the supervision of: \\vfill Prof. Joe Smith and Dr. Frank Smith"
# Entry5: "Stellenbosch University"
# Entry6: April 2020
# Entry7:
# Entry8:

# --------- Front Page
# Comment: ----- Follow this pattern for up to 5 authors
AddTitle: TRUE # Use FALSE when submitting to peer reviewed platform. This will remove author names.
Author1: "Ruan Geldenhuys" # First Author - note the thanks message displayed as an italic footnote of first page.
Ref1: "Stellenbosch University, Stellenbosch, South Africa" # First Author's Affiliation
Email1: "22550801\\@sun.ac.za" # First Author's Email address

Author2: "Nico Katzke"
Ref2: "Stellenbosch University, Stellenbosch, South Africa"
Email2: "nfkatzke\\@gmail.com"
CommonAffiliation_12: TRUE # If Author 1 and 2 have a common affiliation. Works with _13, _23, etc.

# If corresponding author is author 3, e.g., use CorrespAuthor_3: TRUE

# Comment out below to remove both. JEL Codes only given if keywords also given.
keywords: "Multivariate GARCH \\sep Spillovers" # Use \\sep to separate

# ----- Manage headers and footers:
#BottomLFooter: $Title$
#BottomCFooter:
#TopLHeader: \leftmark # Adds section name at topleft. Remove comment to add it.
BottomRFooter: "\\footnotesize Page \\thepage" # Add a '#' before this line to remove footer.
addtoprule: TRUE
addfootrule: TRUE               # Use if footers added. Add '#' to remove line.

# --------- page margins:
margin: 2.3 # Sides
bottom: 2 # bottom
top: 2.5 # Top
HardSet_layout: TRUE # Hard-set the spacing of words in your document. This will stop LaTeX squashing text to fit on pages, e.g.
# This is done by hard-setting the spacing dimensions. Set to FALSE if you want LaTeX to optimize this for your paper.

# --------- Line numbers
linenumbers: FALSE # Used when submitting to journal

# ---------- References settings:
# You can download cls format here: https://www.zotero.org/ - simply search for your institution. You can also edit and save cls formats here: https://editor.citationstyles.org/about/
# Hit download, store it in Tex/ folder, and change reference below - easy.
bibliography: Tex/ref.bib       # Do not edit: Keep this naming convention and location.
csl: Tex/harvard-stellenbosch-university.csl # referencing format used.
# By default, the bibliography only displays the cited references. If you want to change this, you can comment out one of the following:
#nocite: '@*' # Add all items in bibliography, whether cited or not
# nocite: |  # add specific references that aren't cited
#  @grinold2000
#  @Someoneelse2010

# ---------- General:
RemovePreprintSubmittedTo: TRUE  # Removes the 'preprint submitted to...' at bottom of titlepage
Journal: "Journal of Finance"   # Journal that the paper will be submitting to, if RemovePreprintSubmittedTo is set to TRUE.
toc: FALSE                       # Add a table of contents
numbersections: TRUE             # Should sections (and thus figures and tables) be numbered?
fontsize: 11pt                  # Set fontsize
linestretch: 1.2                # Set distance between lines.
link-citations: TRUE            # This creates dynamic links to the papers in reference list.

### Adding additional latex packages:
# header-includes:
#    - \usepackage{colortbl} # Add additional packages here.

output:
  pdf_document:
    keep_tex: TRUE
    template: Tex/TexDefault.txt
    fig_width: 3.5 # Adjust default figure sizes. This can also be done in the chunks of the text.
    fig_height: 3.5
abstract: |
  I investigate the relationship between the S\&P 500 and the JSE Top 40, through various volatility modeling techniques. The purpose of this study is to investigate if this relationship changes in any significant way during the two biggest crisis periods in the last two decades, namely the Global Financial Crisis and Covid-19. I first do a stratification analysis which reveals significant evidence of these two indices sharing periods of high volatility. I then fit multiple multivariate GARCH models to further investigate the relationship and find conditional correlation to increase during crisis periods. Lastly, an investigation of volatility linkages reveals no evidence of volatility spillovers.
---

<!-- First: Set your default preferences for chunk options: -->

<!-- If you want a chunk's code to be printed, set echo = TRUE. message = FALSE stops R printing ugly package loading details in your final paper too. I also suggest setting warning = FALSE and checking for warnings in R, else you might find ugly warnings in your paper. -->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, fig.width = 6, fig.height = 5, fig.pos="H", fig.pos = 'H')

rm(list = ls()) # Clean your environment:
gc() # garbage collection - It can be useful to call gc after a large object has been removed, as this may prompt R to return memory to the operating system.
library(tidyverse)
library(readxl)
library(fmxdat)
library(tseries)
library(knitr)
library(MTS)
library(zoo)
library(ggthemes)
library(rmgarch)
library(rugarch)
library(mgarchBEKK)
library(tikzDevice)
library(moments)
library(xtable)

list.files('code/', full.names = T, recursive = T) %>% .[grepl('.R', .)] %>% as.list() %>% walk(~source(.))

```

<!-- ############################## -->

<!-- # Start Writing here: -->

<!-- ############################## -->

# Introduction \label{Introduction}

In my honours thesis I investigate spillover effects from the S\&P 500 to the JSE Top 40, utilising a Structural VAR. Subsequently, in a time series econometrics essay I applied similar analysis, this time estimating a Bayesian VAR. Now, I turn to various volatility modeling methods in order to uncover new insights into the dynamics between these stock indices. The US stock market's position as a market leader is a well established fact within the literature. It therefore becomes imperative to understand our own local stock market's relationship with the US, especially in times of crisis. 

# Data

Three return series are used in the analysis that follows. These are the monthly returns for the S\&P 500 and the JSE Top 40, as well as thhe ZAR/USD exchange rate. The exchange rate is represented as the amount of Rands required to buy one US Dollar. Since the series is represented as a growth rate, a postive growth rate represents a depreciation of the Rand, and conversely, an appreciation of the Dollar. The inclusion of an exchange rate serves primarily as a control variable and as such analysis regarding the exchange rate is kept to a minimum in the final analysis. The returns for these 3 series' are visualised below in Figures \ref{Figure1} to \ref{Figure3}.

```{r Figure1, warning =  FALSE, fig.align = 'center', fig.cap = "S\\&P 500 Returns \\label{Figure1}", fig.ext = 'png', fig.height = 5, fig.width = 6}

global_indices <- readRDS("data/Global_Indices.rds")
local_indices <- readRDS("data/LCL_Indices.rds")
USDZAR <- readRDS("data/USDZAR.rds")

SP <- global_indices %>% #This includes rand returns
    filter(Tickers == "SPXT") %>% 
    select(c(date, Returns)) %>% 
    rename(SP500 = Returns)

lcl_index <- "J200" # I create this variable so the choice of SA index can easily be changed
JSE <- local_indices %>% 
    filter(Tickers == lcl_index) %>% 
    select(c(date, Returns)) %>% 
    rename(JSE40 = Returns)

joinedDF <- left_join(SP, JSE, by = 'date')

firstdate <- joinedDF %>%  slice(1) %>% pull(date)

ZARUSD <- USDZAR %>% 
    select(c(date, value)) %>% 
    filter(date >= firstdate) %>% 
    mutate(yearmonth = format(ymd(date), "%Y-%m")) %>% 
    group_by(yearmonth) %>% 
    mutate(ZARUSD = dplyr::last(value)/dplyr::first(value) - 1) %>% 
    filter(date == dplyr::last(date)) %>% 
    ungroup() %>% 
    slice(-1) %>% 
    select(c(date, ZARUSD))

joinedDF <- left_join(ZARUSD, joinedDF, by = 'date') 
joinedDF <- joinedDF[c(1,3,4,2)]

#Plot the returns
returnplots <- returns_plotter(joinedDF, c("S&P 500", "JSE Top 40", "USD/ZAR"))
returnplots$`S&P 500`


```

```{r Figure2, warning =  FALSE, fig.align = 'center', fig.cap = "JSE Top 40 Returns \\label{Figure2}", fig.ext = 'png', fig.height = 5, fig.width = 6}

returnplots$`JSE Top 40`

```

Not much information can be revealed through simply observing the returns over time. However, when investigating the squared returns as a measure of volatility, it is clear to see that the JSE Top 40 is substantially more volatile than the S\&P 500. This result is reinforced by Table \ref{tab1}, where the JSE showcases a standard deviation considerably higher than that of the S\&P. Interestingly, the JSE Top 40 showcases higher average monthly returns, however that comes at the cost of the increased volatility as described above. Lastly, as shown in Table \ref{tab1} the S\&P experienced the largest draw down, while the JSE experienced the largest uptick.

```{r Figure3, warning =  FALSE, fig.align = 'center', fig.cap = "ZAR/USD Returns \\label{Figure3}", fig.ext = 'png', fig.height = 5, fig.width = 6}

returnplots$`USD/ZAR`

```

```{r, results='asis'}
sumstats <- function(vec){
    mean <- mean(vec)
    median <- median(vec)
    sd <- sd(vec)
    kurtosis <- kurtosis(vec)
    skewness <- skewness(vec)
    minimum <- min(vec)
    maximum <- max(vec)
    
    result_vec <- c(mean, median, sd, kurtosis, skewness, minimum, maximum)
    return(result_vec)
}

SP_stats <- sumstats(joinedDF$SP500)
JSE_stats <- sumstats(joinedDF$JSE40)
ZARUSD_stats <- sumstats(joinedDF$ZARUSD)

stats_df <- data.frame(
    Measure = c("Mean", "Median", "Std. Dev.", "Kurtosis", "Skewness", "Minimum", "Maximum"),
    SP500 = SP_stats,
    JSE40 = JSE_stats,
    ZARUSD = ZARUSD_stats
)
colnames(stats_df) <- c("", "S&P 500", "JSE Top 40", "ZAR/USD")

table <- xtable(stats_df, caption = "Summary Statistics \\label{tab1}", digits = c(0,0,4,4,4))

  print.xtable(table,
             # tabular.environment = "longtable",
             floating = TRUE,
             table.placement = 'H',
             # scalebox = 0.3,
             comment = FALSE,
             caption.placement = 'top',
             include.rownames = FALSE
             )

```

Before GARCH models can be fitted, ARCH tests need to be conducted in order to see if controlling for conditional heteroskedasticity is appropriate. I employ two tests. First, a univariate Ljung-Box test is conducted on each series. Practically, to test for ARCH effects a simple AR(1) model is fitted for each series and then Ljung-Box tests are done on the residuals of each AR(1). Next, multivariate Portmanteau tests are conducted to incorporate all variables simultaneously. As outlined by @Tsay2014, 3 tests are tun. The results can found in the tables below

```{r, results='asis'}
ret_df <- joinedDF %>% 
    select(c(-date))


ljungbox_tests <- function(df) {
  results <- data.frame(Series = character(),
                        TestStatistic = numeric(),
                        PValue = numeric(),
                        LagOrder = numeric(),
                        stringsAsFactors = FALSE)

  for (series in names(df)) {
    # Fit AR(1) model
    model <- lm(df[[series]] ~ lag(df[[series]]), data = df, na.action = na.exclude)

    # Perform Ljung-Box test on squared residuals
    test_result <- Box.test(residuals(model)^2, lag = 12, type = "Ljung-Box", fitdf = 1)

    # Compile results
    results <- rbind(results, data.frame(Series = series,
                                         TestStatistic = test_result$statistic,
                                         PValue = test_result$p.value,
                                         LagOrder = 12))
  }

  rownames(results) <- NULL

  return(results)
}


arch_results_lb <- ljungbox_tests(ret_df)
table <- xtable(arch_results_lb, caption = "Ljung-Box Tests \\label{tab2}", digits = c(0,0,4,4,0))
print.xtable(table,
             # tabular.environment = "longtable",
             floating = TRUE,
             table.placement = 'H',
             # scalebox = 0.3,
             comment = FALSE,
             caption.placement = 'top',
             include.rownames = FALSE
             )


```
```{r, include=FALSE}
arch_results_march <- MarchTest(ret_df)
```
```{r, results='asis'}
march_df <- data.frame(
    Test <- c( "Q(m) of squared series(LM test)", "Rank-based Test", "Q_k(m) of squared series:"),
    TestStat <- c( 81.3244, 92.5271, 165.2131),
    pval <- c( 0.0001, 0.0001, 0.0001)
)
colnames(march_df)<- c("", "Test Statistic", "p-value")

table <- xtable(march_df, caption = "MV Portmanteau Tests \\label{tab3}", digits = c(0,0,4,4))
print.xtable(table,
             # tabular.environment = "longtable",
             floating = TRUE,
             table.placement = 'H',
             # scalebox = 0.3,
             comment = FALSE,
             caption.placement = 'top',
             include.rownames = FALSE
             )


```
As can be seen in Table \ref{tab2}, for the S\&P 500 and the JSE Top 40, the p-values are functionally zero. This means that we can reject the null of no ARCH effects for these series. The same does not hold for ZAR/USD exchange rate, which has a p-value greater than the critical level of 0.05. However when conducting multivariate tests, all tests report p-values that are functionally zero. As such the analysis continues with the assumption that ARCH effects are present within the data. This serves as motivation for the use of GARCH models in this essay.

# Methodology

I first perform stratification analysis on all three series to determine if periods of high or low volatility are shared across the S\&P, the JSE and the ZAR/USD exchange rate. I then fit multiple univariate GARCH models on all three variables to determine an appropriate specification for the multivariate models to come. I then fit three multivariate GARCH models, namely a DCC model, a Go-GARCH model, and a BEKK-GARCH model. Formal definitions and explanations of these models follow below.

## DCC GARCH

Dynamic Conditional Correlation (DCC) models, developed by @Engle2002, are a class of multivariate GARCH models that allow for time varying correlation between variables. This is especially useful to study how specific time periods, like crises, affect the relationship between different stock indices and financial variables. Consider the following GARCH(1,1) model:

\begin{equation}
R_{it} = \mu_i + \epsilon_{it}, \quad \epsilon_{it} = \sigma_{it}z_{it}, \quad z_{it} \sim N(0,1) \label{eq1}
\end{equation}

In the equation above, $R_t = (R_{1t}, R_{2t}, ... R_{nt})$, for $n$ assets/variables, is a vector of asset returns at time $t$. Here $\mu_i$ is the mean return, $\epsilon_{it}$ is the residual, $\sigma_{it}^2$ is the conditional variance and $z_{it}$ is the standard normal innovation. The conditional variance $\sigma_{it}^2$ is modeled as:

\begin{equation}
\sigma_{it}^2 = \alpha_0 + \alpha_1 \epsilon_{it-1}^2 + \beta_1 \sigma_{it-1}^2 \label{eq2}
\end{equation}

To specify the DCC model, the standardized residuals are defined as $\tilde{\epsilon_t} = \left( \epsilon_{1t} / \sigma_{1t}, \ldots, \epsilon_{nt} / \sigma_{nt} \right)'$. The correlation matrix of $\tilde{\epsilon_t}$, denoted by $Q_t$, evolves over time as: 

\begin{equation}
Q_t = \bar{Q}(1 - a - b) + a \tilde{\epsilon}_{t-1}  \tilde{\epsilon}'_{t-1} + b Q_{t-1} \label{eq3}
\end{equation}

where $\bar{Q}$ is the unconditional correlation matrix of $\tilde{\epsilon_t}$. Parameters, $a$ and $b$ are positive and adhere to $a + b < 1$ to ensure stationarity. To obtain the dynamic conditional correlation, the elements of $Q_t$ are standardized. 

In order to estimate a DCC GARCH model, two steps are followed. First, to obtain $\sigma_{it}$ and $\tilde{\epsilon}_t$, a univariate GARCH is fitted for each return series. Then, secondly, the DCC parameters $a$ and $b$ are estimated using a likelihood function derived from the conditional multivariate distribution of $\tilde{\epsilon}_t$.

## GO-GARCH

Generalized Orthogonalized (GO) GARCH models are another class of multivariate GARCH models. Developed by @VanDerWeide2002, the model is based on the assumption that asset returns can be decomposed into orthogonal components, thus simplifying the modeling of their covariance structure. Note that GO-GARCH models can become computationally intensive quickly, as the number of variables in the model increases. Once again consider a a set of $n$ asset returns,  $R_t = (R_{1t}, R_{2t}, ... R_{nt})$. These returns can be expressed as a linear combination of its orthogonal components:

\begin{equation}
R_t = B_t F_t \label{eq4}
\end{equation}

Here $B_t$ is a time-varying $n \times n$ matrix of loadings and  $F_t$ are the orthogonal components, $F_t = (F_{1t}, F_{2t}, ... F_{nt})'$, that are assumed to follow a univariate GARCH process:

\begin{equation}
F_{it} = \sigma_{it}z_{it} \quad z_{it} \sim N(0,1)\label{eq5}
\end{equation}

where $\sigma^2_{it}$ is the conditional variance of $F_{it}$. In order to estimate the model, the loading matrix $B_t$ is estimated based on the observed correlation of the series', while the volatilities of the orthogonal components, $\sigma^2_{it}$, are estimated using standard GARCH procedures. 

## BEKK-GARCH

The BEKK-GARCH models, initially developed by @Bekk1995, are designed specifically to study spillovers between series. The model's ability to capture dynamic covariances and correlations makes it particularly useful for analyzing the interdependencies in financial markets, especially in the context of crises periods. As before consider $n$ assets with returns, $R_t = (R_{1t}, R_{2t}, ... R_{nt})$. For a GARCH(1,1) these returns are given by:

\begin{equation}
R_t = \mu + \epsilon_t, \quad \epsilon_t = H_t^{1/2} z_t, \quad z_t \sim N(0, I) \label{eq6}
\end{equation}

In Equation \ref{eq6}, as before $\mu$ is the vector of mean returns and $\epsilon_t$ is the vector of residuals. Now, $H_t$ is the conditional covariance matrix and $z_t$ is a vector of i.i.d. standard normal innovations. The conditional covariance matrix $H_t$ is modeled as:

\begin{equation}
H_t = C + A \epsilon_{t-1} \epsilon'_{t-1} A' + B H_{t-1} B' \label{eq7}
\end{equation}

where $C$, $A$ and $B$ are coefficient matrices. Notably, $C$ is a triangular matrix with positive diagonal elements, ensuring that it is positive definite. This attribute removes the need for additional constraints. Lastly, estimation is done via maximum likelihood. For interpretation purposes, the variance of he first asset return can be written as follows:

\begin{align}
\sigma^2_{1,t} = & C(1,1)^2 + A(1,1)^2\mu^2_{1,t-1} + 2A(1,1)A(2,1)\mu_{1,t-1}\mu_{2,t-1} + A(2,1)^2\mu^2_{2,t-1} \label{eq8}\\
& + B(1,1)^2\sigma^2_{1,t-1} + 2B(1,1)B(2,1)\sigma_{1,t-1} + B(2,1)^2\sigma^2_{2,t-1} 
\end{align}

# Results

Like stated above, I first employ stratification analysis. I then fit three multivariate GARCH models. The univariate specification these models are based on are selected by fitting different specifications and selecting the best one based on various selection criteria. These results show a gjrGARCH to be the best specification. \footnote{These results are not reported in this document since they serve little purpose other than model construction. For a full table showing the test results see : \url{https://github.com/RuanGeldenhuys/fmx_project_22550801}}.

## Stratification

Stratification analyses allows for the investigation of a particular assets volatility during a particular period of volatility of another asset. While such analysis does not lend itself to causal interpretation regarding volatility spillovers, it does paint a picture of whether the indices that are being investigated tend to have periods of high or low volatility at the same time. In turn, this could point to a direction of interconnectedness between markets which can then be revealed through more robust analysis. The stratification analyses follows below.


```{r, warning=FALSE, results='asis'}
#Winsorizing the data to reduce influence of extreme returns
Idxs <- joinedDF %>% 
    gather(Index, Returns, -date) %>% 
    mutate(Year = format(date, "%Y")) %>% 
    group_by(Index) %>% 
    mutate(Top = quantile(Returns, 0.99), Bot = quantile(Returns, 0.01)) %>% 
    mutate(Returns = ifelse(Returns > Top, Top, 
                         ifelse(Returns < Bot, Bot, Returns))) %>% 
    ungroup()

#The analyze_volatility_periods function returns a table for stratification analysis
# of high and low volatility periods for a specific series

results_SP <- analyze_volatility_periods(joinedDF, "SP500", Idxs)
table <- xtable(results_SP$HighVol, caption = "S\\&P 500 High Volatility \\label{tab4}")
print.xtable(table,
             # tabular.environment = "longtable",
             floating = TRUE,
             table.placement = 'H',
             # scalebox = 0.3,
             comment = FALSE,
             caption.placement = 'top',
             include.rownames = FALSE
             )
table <- xtable(results_SP$LowVol, caption = "S\\&P 500 Low Volatility \\label{tab5}")
print.xtable(table,
             # tabular.environment = "longtable",
             floating = TRUE,
             table.placement = 'H',
             # scalebox = 0.3,
             comment = FALSE,
             caption.placement = 'top',
             include.rownames = FALSE
             )



# results_rand <- analyze_volatility_periods(joinedDF, "ZARUSD", Idxs)
# kableExtra::kable(results_rand$HighVol, caption = "ZAR/USD")
# kableExtra::kable(results_rand$LowVol, caption = "ZAR/USD")
# 
# results_JSE <- analyze_volatility_periods(joinedDF, "JSE40", Idxs)
# kableExtra::kable(results_JSE$HighVol, caption = "JSE Top 40")
# kableExtra::kable(results_JSE$LowVol, caption = "JSE Top 40")



```

Tables \ref{tab4} and \ref{tab5} showcase the stratification of high and low volatility of the S\&P 500. The "SD" column report volatility during that particular period while the "Full\_SD" column shows the volatility for the entire sample period. As such a ratio greater than 1 indicates that particular index or series has a higher than usual volatility in a given period. Analysing periods of high volatility of the S\&P show both the JSE and the ZAR/USD also have significantly higher volatility in these periods, indicated by the ratio > 1. The JSE also reports lower volatility in periods where the S\&P 500 is less volatile, although the same is not true for the exchange rate. 

```{r, results='asis'}
results_JSE <- analyze_volatility_periods(joinedDF, "JSE40", Idxs)
table <- xtable(results_JSE$HighVol, caption = "JSE Top 40 High Volatility \\label{tab6}")
print.xtable(table,
             # tabular.environment = "longtable",
             floating = TRUE,
             table.placement = 'H',
             # scalebox = 0.3,
             comment = FALSE,
             caption.placement = 'top',
             include.rownames = FALSE
             )
table <- xtable(results_JSE$LowVol, caption = "JSE Top 40 Low Volatility \\label{tab7}")
print.xtable(table,
             # tabular.environment = "longtable",
             floating = TRUE,
             table.placement = 'H',
             # scalebox = 0.3,
             comment = FALSE,
             caption.placement = 'top',
             include.rownames = FALSE
             )

```

Tables \ref{tab6} and \ref{tab7} now report the stratification for the JSE Top 40. For high volatility periods, the relationship holds with both the S\&P 500 and the exchange rate showcasing higher than average volatility. In low volatility periods, again, both the S\&P and the ZAR/USD report lower than usual volatility. Like this stated earlier, this does not allow for causal interpretation, but it does point to the fact that the two indices tend to be in periods of high or low volatility at the same time.

```{r, results='asis'}
results_rand <- analyze_volatility_periods(joinedDF, "ZARUSD", Idxs)
table <- xtable(results_rand$HighVol, caption = "ZAR/USD High Volatility \\label{tab8}")
print.xtable(table,
             # tabular.environment = "longtable",
             floating = TRUE,
             table.placement = 'H',
             # scalebox = 0.3,
             comment = FALSE,
             caption.placement = 'top',
             include.rownames = FALSE
             )
table <- xtable(results_rand$LowVol, caption = "ZAR/USD Low Volatility \\label{tab9}")
print.xtable(table,
             # tabular.environment = "longtable",
             floating = TRUE,
             table.placement = 'H',
             # scalebox = 0.3,
             comment = FALSE,
             caption.placement = 'top',
             include.rownames = FALSE
             )

```

Lastly, analysing stratification of the ZAR/USD reveals an interesting result. In periods of high volatility (Table \ref{tab8}), both the JSE and S\&P also report higher volatility. However this volatility is only slighter higher, with both indices reporting a standard deviation that is only 0.01 larger than the full sample. Conversely, in periods of low volatility (Table \ref{tab9}), these indices show significant lower volatility as well. This indicates that a highly volatile Rand does not necessarily mean volatile stock markets, however a low volatile Rand tends to be associated with low volatility in these indices.

## DCC

```{r, include=FALSE}
garch_xts <- joinedDF %>% 
    rename(SP = SP500,
           JSE = JSE40,
           ZARUSD = ZARUSD) %>% 
    tbl2xts::tbl_xts()

uspec <- ugarchspec(variance.model = list(model = "gjrGARCH", garchOrder = c(1, 1)), 
                    mean.model = list(armaOrder = c(1, 0), include.mean = TRUE), 
                    distribution.model = "sstd")

multi_univ_garch_spec <- multispec(replicate(ncol(garch_xts), uspec))

spec.dcc = dccspec(multi_univ_garch_spec, dccOrder = c(1, 1), distribution = "mvnorm", 
                   lag.criterion = c("AIC", "HQ", "SC", "FPE")[1], 
                   model = c("DCC", "aDCC")[1])

cl = makePSOCKcluster(10)

multf = multifit(multi_univ_garch_spec, garch_xts, cluster = cl)

fit.dcc = dccfit(spec.dcc, data = garch_xts, solver = "solnp", 
    cluster = cl, fit.control = list(eval.se = FALSE), fit = multf)

RcovList <- rcov(fit.dcc)  # This is now a list of the monthly covariances of our DCC model series.
covmat = matrix(RcovList, nrow(garch_xts), ncol(garch_xts) * ncol(garch_xts), 
    byrow = TRUE)
mc1 = MCHdiag(garch_xts, covmat)

dcc.time.var.cor <- rcor(fit.dcc)

dcc.time.var.cor <- aperm(dcc.time.var.cor, c(3, 2, 1))
dim(dcc.time.var.cor) <- c(nrow(dcc.time.var.cor), ncol(dcc.time.var.cor)^2)

dcc.time.var.cor <- renamingdcc(ReturnSeries = garch_xts, DCC.TV.Cor = dcc.time.var.cor)

```


```{r Figure4, warning =  FALSE, fig.align = 'center', fig.cap = "DCC GARCH \\label{Figure4}", fig.ext = 'png', fig.height = 5, fig.width = 6}
dcc_JSE_gjr <- ggplot(dcc.time.var.cor %>% dplyr::filter(grepl("JSE_", Pairs ), !grepl("_JSE", Pairs)) ) + 
    geom_line(aes(x = date, y = Rho, colour = Pairs), linewidth = 1) + 
    
    annotate("rect", xmin = as.Date("2007-06-22"), xmax = as.Date("2009-06-23"),
    ymin = -Inf, ymax = Inf, fill = "red", alpha = 0.4)+
    annotate("rect", xmin = as.Date("2020-03-15"), xmax = as.Date("2022-06-20"),
    ymin = -Inf, ymax = Inf, fill = "blue", alpha = 0.4)+
    
    theme_fmx()+
    ggtitle("Dynamic Conditional Correlations: JSE")+
    scale_x_date(date_breaks = "4 years", date_labels = "%Y")

finplot(dcc_JSE_gjr)
```

The dynamic conditional correlation for the JSE Top 40, as reported by the DCC-GARCH model, are shown in Figure \ref{Figure4}. Intuitively, it shows a noise reduced correlation between the JSE and other variables in the system over time. What is immediately clear is that the JSE shares a significantly higher correlation with the S\&P 500, than it does with the ZAR/USD exchange rate. In fact, the correlation with the ZAR/USD is negative for much of the sample period. This result does make sense since "increases" in the exchange rate indicate a depreciation of the Rand.

Analyzing the crisis periods, particularly in the context of the correlation between the JSE Top 40 and the S\&P 500, reveals an interesting result. Note, the GFC period is indicated by the red shaded area, while Covid-19 is shown in blue. During the GFC the correlation jumps up initially, then turns sharply downward, before tending upward for the rest of the crisis. The difference in correlation between the start of the crises and the maximum correlation is equal to 0.075, while the difference between the start and end of the crisis is equal to 0.063.

Correlation during Covid-19 behaves differently. Here the model reports a sharp increase in correlation that fades out as the crisis draws to a close. As such the maximum correlation appears early in the crisis period. The difference in correlation between the start of the crisis period and the maximum is equal to 0.115. The difference between the start and end of the crisis is smaller at 0.010.

```{r, include=FALSE}
# This code extracts the correlations for the GFC and COvid-19.
# It then calculates the difference between the first and max, and the first and
# last correlation.

## GFC calculations
gfc_df <- dcc.time.var.cor %>% 
    filter(Pairs == "JSE_SP") %>% 
    filter(date >= as.Date("2007-06-22") & date <= as.Date("2009-06-23"))

first_gfc <- gfc_df %>% slice(1) %>% pull(Rho)
last_gfc <- gfc_df %>% slice(n()) %>% pull(Rho)
max_gfc <- max(gfc_df$Rho)

max_gfc - first_gfc
last_gfc - first_gfc


## Covid-19 calculations
covid_df <- dcc.time.var.cor %>% 
    filter(Pairs == "JSE_SP") %>% 
    filter(date >= as.Date("2020-02-15") & date <= as.Date("2021-12-01"))

first_covid <- covid_df %>% slice(1) %>% pull(Rho)
last_covid <- covid_df %>% slice(n()) %>% pull(Rho)
max_covid <- max(covid_df$Rho)

max_covid - first_covid
last_covid - first_covid

```


## Go-GARCH

```{r, include=FALSE}
spec.go <- gogarchspec(multi_univ_garch_spec, 
                       distribution.model = 'mvnorm', # or manig.
                       ica = 'fastica') # Note: we use the fastICA
cl <- makePSOCKcluster(10)
multf <- multifit(multi_univ_garch_spec, garch_xts, cluster = cl)

fit.gogarch <- gogarchfit(spec.go, 
                      data = garch_xts, 
                      solver = 'hybrid', 
                      cluster = cl, 
                      gfun = 'tanh', 
                      maxiter1 = 40000, 
                      epsilon = 1e-08, 
                      rseed = 100)

print(fit.gogarch)

# Extracting time-varying conditional correlations: You know the drill...
gog.time.var.cor <- rcor(fit.gogarch)
gog.time.var.cor <- aperm(gog.time.var.cor,c(3,2,1))
dim(gog.time.var.cor) <- c(nrow(gog.time.var.cor), ncol(gog.time.var.cor)^2)
# Finally:
gog.time.var.cor <-
renamingdcc(ReturnSeries = garch_xts, DCC.TV.Cor = gog.time.var.cor)
```

```{r Figure5, warning =  FALSE, fig.align = 'center', fig.cap = "GO-GARCH \\label{Figure5}", fig.ext = 'png', fig.height = 5, fig.width = 6}

gog_JSE_gjr <- ggplot(gog.time.var.cor %>% dplyr::filter(grepl("JSE_", Pairs ), !grepl("_JSE", Pairs)) ) + 
    geom_line(aes(x = date, y = Rho, colour = Pairs), linewidth = 1) + 
    
    annotate("rect", xmin = as.Date("2007-06-22"), xmax = as.Date("2009-06-23"),
    ymin = -Inf, ymax = Inf, fill = "red", alpha = 0.4)+
    annotate("rect", xmin = as.Date("2020-03-15"), xmax = as.Date("2022-06-20"),
    ymin = -Inf, ymax = Inf, fill = "blue", alpha = 0.4)+
    
    theme_fmx()+
    ggtitle("Dynamic Conditional Correlations: JSE")+
    scale_x_date(date_breaks = "4 years", date_labels = "%Y")

finplot(gog_JSE_gjr)


```

The dynamic conditional correlations for the JSE Top 40, as estimated by the GO-GARCH model is reported in Figure \ref{Figure5}. It is immediately apparent that the correlation with the exchange rate is much more volatile than in the DCC model. It is still negative for most of the sample, however experiences large spikes, turning the relationship positive. The correlation with the S\&P 500 follows a similar time path to the DCC model, in crises periods. As such the GO-GARCH serves as a robustness check for the results found by the DCC model. 

A key difference between the model is the fact that the jump in correlation during crisis periods are more pronounced. The difference in correlation between the start and maximum of the GFC is equal to 0.362, while the difference between the first and last correlation is equal to 0.219. For Covid-19, the difference between the first correlation and maximum correlation is 0.278, and interestingly the difference between the first and last is negative at -0.011.

```{r, include=FALSE}
# This code extracts the correlations for the GFC and COvid-19.
# It then calculates the difference between the first and max, and the first and
# last correlation.

## GFC calculations
gfc_df <- gog.time.var.cor %>% 
    filter(Pairs == "JSE_SP") %>% 
    filter(date >= as.Date("2007-06-22") & date <= as.Date("2009-06-23"))

first_gfc <- gfc_df %>% slice(1) %>% pull(Rho)
last_gfc <- gfc_df %>% slice(n()) %>% pull(Rho)
max_gfc <- max(gfc_df$Rho)

max_gfc - first_gfc
last_gfc - first_gfc


## Covid-19 calculations
covid_df <- gog.time.var.cor %>% 
    filter(Pairs == "JSE_SP") %>% 
    filter(date >= as.Date("2020-02-15") & date <= as.Date("2021-12-01"))

first_covid <- covid_df %>% slice(1) %>% pull(Rho)
last_covid <- covid_df %>% slice(n()) %>% pull(Rho)
max_covid <- max(covid_df$Rho)

max_covid - first_covid
last_covid - first_covid
```

## BEKK-GARCH

The estimated parameters for the BEKK-GARCH model can be found in Table \ref{tab10} below.

```{r, include=FALSE}
garch_df <- joinedDF %>% 
    select(c(SP500, JSE40, ZARUSD)) %>% 
    rename(SP = SP500,
           JSE = JSE40,
           ZARUSD = ZARUSD) 

garch_matrix <- as.matrix(garch_df)
estimated <- mgarchBEKK::BEKK(garch_matrix)
```

```{r, results='asis'}
extract_matrix_elements <- function(matrix, prefix) {
  # Initialize an empty dataframe with appropriate column names
  result_df <- data.frame(Position = character(), Value = numeric(), stringsAsFactors = FALSE)

  # Loop through each element of the matrix
  for (row in 1:nrow(matrix)) {
    for (col in 1:ncol(matrix)) {
      # Create a position string (e.g., "(2,1)")
      position <- paste(prefix ,"(", row, ",", col, ")", sep = "")
      
      # Append the position and value to the dataframe
      result_df <- rbind(result_df, data.frame(Position = position, Value = matrix[row, col]))
    }
  }
  
  return(result_df)
}

garch_constants <- extract_matrix_elements(estimated$est.params[[1]], "C")
garch_constants_se <- extract_matrix_elements(estimated$asy.se.coef[[1]], "C")

garch_result_df <- left_join(garch_constants, garch_constants_se, by="Position") %>% 
    filter(Value.x != 0)

garch_amat <- extract_matrix_elements(estimated$est.params[[2]], "A")
garch_amat_se <- extract_matrix_elements(estimated$asy.se.coef[[2]], "A")

amat_joined <- left_join(garch_amat, garch_amat_se, by="Position")

garch_bmat <- extract_matrix_elements(estimated$est.params[[3]], "B")
garch_bmat_se <- extract_matrix_elements(estimated$asy.se.coef[[3]], "B")

bmat_joined <- left_join(garch_bmat, garch_bmat_se, by="Position")

garch_result_df <- rbind(garch_result_df, amat_joined)
garch_result_df <- rbind(garch_result_df, bmat_joined)
colnames(garch_result_df) <- c(" ", "Coefficient", "Std. Error")

garch_results_final <- garch_result_df %>% 
    mutate(TStat = Coefficient / `Std. Error`,
           Significance = ifelse(abs(TStat) > 2.576, "***", 
                                 ifelse(abs(TStat) > 1.96, "**", 
                                        ifelse(abs(TStat) > 1.645, "*", ""))))

table <- xtable(garch_results_final, caption = "BEKK-GARCH Results \\label{tab10}", digits = c(0,0,4,4,4,0))
print.xtable(table,
             # tabular.environment = "longtable",
             floating = TRUE,
             table.placement = 'H',
             # scalebox = 0.3,
             comment = FALSE,
             caption.placement = 'top',
             include.rownames = FALSE
             )

```

Since the main goal of estimating a BEKK-GARCH model, in this essay, is to measure volatility spillovers, I focus on the B matrix of parameters as defined in Equation \ref{eq7}. Recall, Equation \ref{eq8} for details on interpretation. In this model the S\&P 500 is variable 1, the JSE Top 40 is variable 2, and the ZAR/USD exchange rate is variable 3. This means that coefficient of B(1,2) represents the volatility spillover from the S\&P 500 to the JSE Top 40. 

An interesting result the model produces is that no volatility spillovers between stock indices are statistically significant. The only significant volatility spillover appears to be B(3,2), representing a spillover from the ZAR/USD to the JSE Top 40. This result is peculiar to say the least. One would expect there to exists some level of volatility spillovers between stock indices, especially from large markets like the S\&P 500 to significantly smaller markets like the JSE, as the literature has shown. Earlier models showed that a relationship between these indices do in fact exist, however this model was incapable of revealing such relationship between the volatilities. 

# Conclusion

In summary, I find the S\&P 500 and the JSE Top 40 to largely share periods of high and low volatility. These markets are highly correlated, as made evident by the DCC and GO-GARCH models. These model further reveal conditional correlation to have spikes during the Global Financial Crises and Covid-19. An analysis of volatility spillovers revealed evidence of spillovers from the ZAR/USD exchange rate, but not between stock indices. This result is perplexing as literature often reveals such relationship between the US and emerging market economies. 

It is important to note that this analysis only included the largest stocks for each stock market. As such, no claims can be made regarding whether the dynamics revealed by these models will hold for lower market cap stocks. Additionally, no cross-country comparisons have been made. A study across multiple emerging markets, not only South Africa, is a likely area of future research. Lastly, if evidence of volatility spillovers is found an extension to time varying parameters could be beneficial in order to study the effect of crisis periods in more detail.

<!-- Make title of bibliography here: -->

<!-- \newpage -->

\newpage

# References {.unnumbered}

::: {#refs}
:::


