---
output:
  md_document:
    variant: markdown_github
---

# Purpose

Purpose of this work folder.

Ideally store a minimum working example data set in data folder.

Add binary files in bin, and closed R functions in code. Human Readable settings files (e.g. csv) should be placed in settings/


```{r}

rm(list = ls()) # Clean your environment:
gc() # garbage collection - It can be useful to call gc after a large object has been removed, as this may prompt R to return memory to the operating system.
library(tidyverse)
library(readxl)
library(fmxdat)
library(tseries)
library(knitr)
library(MTS)
library(zoo)
library(ggthemes)

list.files('code/', full.names = T, recursive = T) %>% .[grepl('.R', .)] %>% as.list() %>% walk(~source(.))
```
# Loading and Wrangling

This section deals with loading and wrangling the data into a usable format.
```{r}
global_indices <- readRDS("data/Global_Indices.rds")
local_indices <- readRDS("data/LCL_Indices.rds")
USDZAR <- readRDS("data/USDZAR.rds")

SP <- global_indices %>% #This includes rand returns
    filter(Tickers == "SPXT") %>% 
    select(c(date, Returns, Rand_Returns)) %>% 
    rename(SP500 = Returns)

lcl_index <- "J200" # I create this variable so the choice of SA index can easily be changed
JSE <- local_indices %>% 
    filter(Tickers == lcl_index) %>% 
    select(c(date, Returns)) %>% 
    rename(JSE40 = Returns)

joinedDF <- left_join(SP, JSE, by = 'date')






returns_plotter(joinedDF, c("S&P 500", "Rand Returns", "JSE Top 40"))


```

# Stratification

This analysis will first focus on seeing whether the JSE experience higher volatility when the S&P and the rand experiences higher volatility. I then investigate whether all variables experienced it during the GFC and Covid. This follows the practical

```{r}
#Winsorizing the data to reduce influence of extreme returns
Idxs <- joinedDF %>% 
    gather(Index, Returns, -date) %>% 
    mutate(Year = format(date, "%Y")) %>% 
    group_by(Index) %>% 
    mutate(Top = quantile(Returns, 0.99), Bot = quantile(Returns, 0.01)) %>% 
    mutate(Returns = ifelse(Returns > Top, Top, 
                         ifelse(Returns < Bot, Bot, Returns))) %>% 
    ungroup()

results_SP <- analyze_volatility_periods(joinedDF, "SP500", Idxs)
kableExtra::kable(results_SP$HighVol)
kableExtra::kable(results_SP$LowVol)

results_rand <- analyze_volatility_periods(joinedDF, "Rand_Returns", Idxs)
kableExtra::kable(results_rand$HighVol)
kableExtra::kable(results_rand$LowVol)

results_JSE <- analyze_volatility_periods(joinedDF, "JSE40", Idxs)
kableExtra::kable(results_JSE$HighVol)
kableExtra::kable(results_JSE$LowVol)



```


# ARCH Tests

To test for ARCH effects I create a function that fits a simple AR(1) to each return series. I then run Ljung-Box tests on the residuals of each of those models. The null of "No ARCH effects" is rejected for all three series.


```{r}
ret_df <- joinedDF %>% 
    select(c(-date))


ljungbox_tests <- function(df) {
  results <- data.frame(Series = character(),
                        TestStatistic = numeric(),
                        PValue = numeric(),
                        LagOrder = numeric(),
                        stringsAsFactors = FALSE)

  for (series in names(df)) {
    # Fit AR(1) model
    model <- lm(df[[series]] ~ lag(df[[series]]), data = df, na.action = na.exclude)

    # Perform Ljung-Box test on squared residuals
    test_result <- Box.test(residuals(model)^2, lag = 12, type = "Ljung-Box", fitdf = 1)

    # Compile results
    results <- rbind(results, data.frame(Series = series,
                                         TestStatistic = test_result$statistic,
                                         PValue = test_result$p.value,
                                         LagOrder = 12))
  }

  rownames(results) <- NULL

  return(results)
}


arch_results <- ljungbox_tests(ret_df)
kable(arch_results)

```



#GARCH modelling
```{r}
library(rmgarch)
library(rugarch)
install.packages("mgarchBEKK")
library(mgarchBEKK)

garch_df <- joinedDF %>% 
    select(c(SP500, JSE40, Rand_Returns)) %>% 
    mutate(SP = SP500 * 100,
           JSE = JSE40 * 100,
           Rand = Rand_Returns * 100) %>% 
    select(c(SP, JSE, Rand))

garch_matrix <- as.matrix(garch_df)

estimated <- mgarchBEKK::BEKK(garch_matrix)
estimated$est.params







```

#Other GARCH models
I will now fit various univariate GARCH models to determine the best specification.

```{r}
uniGarchFitter <- function(data){
    models <- c("sGARCH", "gjrGARCH", "apARCH")
    dist.model <- "norm"
    
    result_list <- list()
    
    #Loop through each column in DF
    for (i in 1:ncol(data)) { 
        resultDF <- data.frame(
            Model = character(),
            Akaike = integer(),
            Bayes = integer(),
            Shibata = integer(),
            HannanQuinn = integer()
        )
        
        j = 0
        
        #For each column loop through each type of model and fit it
        for (model_type in models) {
            j = j+1 #additional counter since we are looping though a string list
            spec <- ugarchspec(
                variance.model = list(model = model_type, garchOrder = c(1, 1)),
                mean.model = list(armaOrder = c(1, 0), include.mean = TRUE),
                distribution.model = dist.model
                )
            
            fit <- ugarchfit(spec = spec, data = as.data.frame(data[i]))
            
            IC <- infocriteria(fit)
            
            resultDF[j, 1] <- model_type #Place the IC for that particular model in the DF
            resultDF[j, 2:5] <- IC
            
            
        }
        
        #add that result DF to the main list
        result_list[[colnames(data)[i]]] <- resultDF 
        
    }
    
    return(result_list)
}


uGarch_tables <- uniGarchFitter(garch_df)

kableExtra::kable(uGarch_tables$SP, caption = "S&P 500")
kableExtra::kable(uGarch_tables$JSE, caption = "JSE Top 40")
kableExtra::kable(uGarch_tables$Rand, caption = "Rand")




```

For the S&P 500 and JSE Top 40 the gjrGARCH performs best. For the Rand it is the apARCH. I therefore select the gjrGARCH as my univariate specification.

#Multivariate GARCH
```{r}
garch_xts <- joinedDF %>% 
    rename(SP = SP500,
           JSE = JSE40,
           Rand = Rand_Returns) %>% 
    tbl2xts::tbl_xts()

uspec <- ugarchspec(variance.model = list(model = "gjrGARCH", 
    garchOrder = c(1, 1)), mean.model = list(armaOrder = c(1, 
    0), include.mean = TRUE), distribution.model = "norm")

mspec <- multispec(replicate(ncol(garch_xts), uspec))

spec.dcc = dccspec(mspec, dccOrder = c(1, 1), 
    distribution = "mvnorm", lag.criterion = c("AIC", "HQ", "SC", 
        "FPE")[1], model = c("DCC", "aDCC")[1])

cl = makePSOCKcluster(10)

mfit = multifit(mspec, garch_xts, cluster = cl)

fit.dcc = dccfit(spec.dcc, data = garch_xts, solver = "solnp", 
    cluster = cl, fit.control = list(eval.se = FALSE), fit = mfit)

fit.dcc$rho.t
```

```{r}
DCCpre <- dccPre(garch_xts, include.mean = T, p = 0)
StdRes <- DCCpre$sresi

DCC <- dccFit(StdRes, type="Engle")

Rhot <- DCC$rho.t

ReturnSeries = garch_xts
DCC.TV.Cor = Rhot

renamingdcc <- function(ReturnSeries, DCC.TV.Cor) {
  
ncolrtn <- ncol(ReturnSeries)
namesrtn <- colnames(ReturnSeries)
paste(namesrtn, collapse = "_")

nam <- c()
xx <- mapply(rep, times = ncolrtn:1, x = namesrtn)
# Now let's be creative in designing a nested for loop to save the names corresponding to the columns of interest.. 

# TIP: draw what you want to achieve on a paper first. Then apply code.

# See if you can do this on your own first.. Then check vs my solution:

nam <- c()
for (j in 1:(ncolrtn)) {
for (i in 1:(ncolrtn)) {
  nam[(i + (j-1)*(ncolrtn))] <- paste(xx[[j]][1], xx[[i]][1], sep="_")
}
}

colnames(DCC.TV.Cor) <- nam

# So to plot all the time-varying correlations wrt SBK:
 # First append the date column that has (again) been removed...
DCC.TV.Cor <- 
    data.frame( cbind( date = index(ReturnSeries), DCC.TV.Cor)) %>% # Add date column which dropped away...
    mutate(date = as.Date(date)) %>%  tbl_df() 

DCC.TV.Cor <- DCC.TV.Cor %>% gather(Pairs, Rho, -date)

DCC.TV.Cor

}

# Let's see if our function works! Excitement!
Rhot <- 
  renamingdcc(ReturnSeries = garch_xts, DCC.TV.Cor = Rhot)

head(Rhot %>% arrange(date))

str(Rhot)

dcc_JSE <- ggplot(Rhot %>% filter(grepl("JSE_", Pairs ), !grepl("_JSE", Pairs)) ) + 
    geom_line(aes(x = date, y = Rho, colour = Pairs)) + 
    
    annotate("rect", xmin = as.Date("2007-06-22"), xmax = as.Date("2009-06-23"),
    ymin = -Inf, ymax = Inf, fill = "red", alpha = 0.4)+
    annotate("rect", xmin = as.Date("2020-03-15"), xmax = as.Date("2022-06-20"),
    ymin = -Inf, ymax = Inf, fill = "blue", alpha = 0.4)+
    
    theme_fmx()+
    ggtitle("Dynamic Conditional Correlations: JSE")

finplot(dcc_JSE)

```

