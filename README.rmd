---
output:
  md_document:
    variant: markdown_github
---

# Purpose

I investigate linkages between the S&P 500 and the JSE Top 40 through stratification and various GARCH modeling techniques.

This README serves as guide to the code for my Financial Econometrics essay. The full essay can be found under "22550801_FMX_essay.pdf". The code used to create the formal document can found under the "Texevier/" folder. All functions used in the code below are stored in separate scripts stored in the "code/" folder. The purpose of this README is therefore to document the code that was used in creating the output used in the essay. As such all analysis is left to the formal document.

I first load all the required packages and functions from the scripts.
```{r, warning=FALSE, message=FALSE, results='hide'}

rm(list = ls()) # Clean your environment:
gc() # garbage collection - It can be useful to call gc after a large object has been removed, as this may prompt R to return memory to the operating system.
library(tidyverse)
library(readxl)
library(fmxdat)
library(tseries)
library(knitr)
library(MTS)
library(zoo)
library(ggthemes)
library(rmgarch)
library(rugarch)
library(mgarchBEKK)

list.files('code/', full.names = T, recursive = T) %>% .[grepl('.R', .)] %>% as.list() %>% walk(~source(.))
```

# Loading and Wrangling

This section deals with loading and wrangling the data into a usable format. I then plot the returns, absolute returns and squared returns using the "returns_plotter" function.

```{r, warning=FALSE}
global_indices <- readRDS("data/Global_Indices.rds")
local_indices <- readRDS("data/LCL_Indices.rds")
USDZAR <- readRDS("data/USDZAR.rds")

#S&P 500
SP <- global_indices %>% 
    filter(Tickers == "SPXT") %>% 
    select(c(date, Returns)) %>% 
    rename(SP500 = Returns)

#JSE Top 40
lcl_index <- "J200" # I create this variable so the choice of SA index can easily be changed
JSE <- local_indices %>% 
    filter(Tickers == lcl_index) %>% 
    select(c(date, Returns)) %>% 
    rename(JSE40 = Returns)

joinedDF <- left_join(SP, JSE, by = 'date')

firstdate <- joinedDF %>%  slice(1) %>% pull(date)

#ZAR/USD exchange rate
ZARUSD <- USDZAR %>% 
    select(c(date, value)) %>% 
    filter(date >= firstdate) %>% 
    mutate(yearmonth = format(ymd(date), "%Y-%m")) %>% 
    group_by(yearmonth) %>% 
    mutate(ZARUSD = dplyr::last(value)/dplyr::first(value) - 1) %>% 
    filter(date == dplyr::last(date)) %>% 
    ungroup() %>% 
    slice(-1) %>% 
    select(c(date, ZARUSD))

joinedDF <- left_join(ZARUSD, joinedDF, by = 'date') 
joinedDF <- joinedDF[c(1,3,4,2)]

#Plot the returns
returns_plotter(joinedDF, c("S&P 500", "JSE Top 40", "USD/ZAR"))


```

# Stratification

This analysis will first focus on seeing whether the JSE experience higher volatility when the S&P and the rand experiences higher volatility. This follows the practical. First returns are winsorized to limit the impact of outliers. The "analyze_volatility_periods" then calculates the high and low volatility periods and subsequent ratios for each series.

```{r, warning=FALSE}
#Winsorizing the data to reduce influence of extreme returns
Idxs <- joinedDF %>% 
    gather(Index, Returns, -date) %>% 
    mutate(Year = format(date, "%Y")) %>% 
    group_by(Index) %>% 
    mutate(Top = quantile(Returns, 0.99), Bot = quantile(Returns, 0.01)) %>% 
    mutate(Returns = ifelse(Returns > Top, Top, 
                         ifelse(Returns < Bot, Bot, Returns))) %>% 
    ungroup()

#The analyze_volatility_periods function returns a table for stratification analysis
# of high and low volatility periods for a specific series

results_SP <- analyze_volatility_periods(joinedDF, "SP500", Idxs)
kableExtra::kable(results_SP$HighVol, caption = "S&P 500")
kableExtra::kable(results_SP$LowVol, caption = "S&P 500")

results_rand <- analyze_volatility_periods(joinedDF, "ZARUSD", Idxs)
kableExtra::kable(results_rand$HighVol, caption = "ZAR/USD")
kableExtra::kable(results_rand$LowVol, caption = "ZAR/USD")

results_JSE <- analyze_volatility_periods(joinedDF, "JSE40", Idxs)
kableExtra::kable(results_JSE$HighVol, caption = "JSE Top 40")
kableExtra::kable(results_JSE$LowVol, caption = "JSE Top 40")



```

# ARCH Tests

To test for ARCH effects I create a function that fits a simple AR(1) to each return series. I then run Ljung-Box tests on the residuals of each of those models. This is handled by the "ljungbox_tests" function. The null of "No ARCH effects" is rejected for the S&P 500 and the JSE Top 40. I then run multivariate Portmanteau tests to incorporate all series, simultaneously. 

```{r}
ret_df <- joinedDF %>% 
    select(c(-date))

#Call function that loops through every series and runs Ljung-Box tests
arch_results_lb <- ljungbox_tests(ret_df) 
kable(arch_results_lb, caption = "Ljung-Box Tests")

#MV Portmanteau tests
arch_results_march <- MarchTest(ret_df)

```

# GARCH modelling

## Univariate GARCH models

I will now fit various univariate GARCH models to determine the best specification. The "uniGarchFitter" function loops through the dataframe and fits 3 different GARCH models to each series. After each type of model is fitted, selection criteria for that model is calculated and added to a dataframe that will later be outputted. 

```{r}


garch_df <- joinedDF %>% 
    select(c(SP500, JSE40, ZARUSD)) %>% 
    rename(SP = SP500,
           JSE = JSE40,
           ZARUSD = ZARUSD) 

uGarch_tables <- uniGarchFitter(garch_df)

kableExtra::kable(uGarch_tables$SP, caption = "S&P 500")
kableExtra::kable(uGarch_tables$JSE, caption = "JSE Top 40")
kableExtra::kable(uGarch_tables$ZARUSD, caption = "ZAR/USD")




```

For the S&P 500 and JSE Top 40 the gjrGARCH performs best. For the Rand it is the sGARCH. I therefore select the gjrGARCH as my univariate specification. 

## Multivariate GARCH

```{r}
#Create and xts object for GARCH modeling to come
garch_xts <- joinedDF %>% 
    rename(SP = SP500,
           JSE = JSE40,
           ZARUSD = ZARUSD) %>% 
    tbl2xts::tbl_xts()

```

### DCC

I first fit an Engle type DCC and then a DCC model based on the univariate gjrGARCH specification. The results are practically the same. For each model I extract the time varying correlation and run it through the "renamingdcc" function to spimplify plotting and analysis later on.

```{r, warning=FALSE}
#Fit DCC
DCCpre <- dccPre(garch_xts, include.mean = T, p = 0)
StdRes <- DCCpre$sresi

DCC <- dccFit(StdRes, type="Engle")

Rhot <- DCC$rho.t

ReturnSeries = garch_xts
DCC.TV.Cor = Rhot


DCC$estimates

#Rename TV correlations
Rhot <- 
  renamingdcc(ReturnSeries = garch_xts, DCC.TV.Cor = Rhot)


#Plot correlations for the JSE
dcc_JSE <- ggplot(Rhot %>% filter(grepl("JSE_", Pairs ), !grepl("_JSE", Pairs)) ) + 
    geom_line(aes(x = date, y = Rho, colour = Pairs), linewidth = 1) + 
    
    annotate("rect", xmin = as.Date("2007-06-22"), xmax = as.Date("2009-06-23"),
    ymin = -Inf, ymax = Inf, fill = "red", alpha = 0.4)+
    annotate("rect", xmin = as.Date("2020-03-15"), xmax = as.Date("2022-06-20"),
    ymin = -Inf, ymax = Inf, fill = "blue", alpha = 0.4)+
    
    theme_fmx()+
    ggtitle("Dynamic Conditional Correlations: JSE")+
    scale_x_date(date_breaks = "4 years", date_labels = "%Y")

finplot(dcc_JSE)

```

```{r, warning=FALSE}
#Spec and fit DCC model based on gjrGARCH specification
uspec <- ugarchspec(variance.model = list(model = "gjrGARCH", garchOrder = c(1, 1)), 
                    mean.model = list(armaOrder = c(1, 0), include.mean = TRUE), 
                    distribution.model = "sstd")

multi_univ_garch_spec <- multispec(replicate(ncol(garch_xts), uspec))

spec.dcc = dccspec(multi_univ_garch_spec, dccOrder = c(1, 1), 
                   distribution = "mvnorm", 
                   lag.criterion = c("AIC", "HQ", "SC", "FPE")[1], 
                   model = c("DCC", "aDCC")[1])

cl = makePSOCKcluster(10) #Enable for speed

multf = multifit(multi_univ_garch_spec, garch_xts, cluster = cl)

fit.dcc = dccfit(spec.dcc, data = garch_xts, solver = "solnp", 
    cluster = cl, fit.control = list(eval.se = FALSE), fit = multf)

#Extract and rename TV correlations
RcovList <- rcov(fit.dcc)  # This is now a list of the monthly covariances of our DCC model series.
covmat = matrix(RcovList, nrow(garch_xts), ncol(garch_xts) * ncol(garch_xts), 
    byrow = TRUE)
#mc1 = MCHdiag(garch_xts, covmat)

dcc.time.var.cor <- rcor(fit.dcc)

dcc.time.var.cor <- aperm(dcc.time.var.cor, c(3, 2, 1))
dim(dcc.time.var.cor) <- c(nrow(dcc.time.var.cor), ncol(dcc.time.var.cor)^2)

dcc.time.var.cor <- renamingdcc(ReturnSeries = garch_xts, DCC.TV.Cor = dcc.time.var.cor)

#Plot correlations for JSE
dcc_JSE_gjr <- ggplot(dcc.time.var.cor %>% dplyr::filter(grepl("JSE_", Pairs ), 
                                                         !grepl("_JSE", Pairs))) + 
    geom_line(aes(x = date, y = Rho, colour = Pairs), linewidth = 1) + 
    
    annotate("rect", xmin = as.Date("2007-06-22"), xmax = as.Date("2009-06-23"),
    ymin = -Inf, ymax = Inf, fill = "red", alpha = 0.4)+
    annotate("rect", xmin = as.Date("2020-03-15"), xmax = as.Date("2022-06-20"),
    ymin = -Inf, ymax = Inf, fill = "blue", alpha = 0.4)+
    
    theme_fmx()+
    ggtitle("Dynamic Conditional Correlations: JSE")+
    scale_x_date(date_breaks = "4 years", date_labels = "%Y")

finplot(dcc_JSE_gjr)


```

Next, to test for changes in correlation during the GFC and Covid-19, I run the following code.

```{r}
# This code extracts the correlations for the GFC and COvid-19.
# It then calculates the difference between the first and max, and the first and
# last correlation.

## GFC calculations
gfc_df <- dcc.time.var.cor %>% 
    filter(Pairs == "JSE_SP") %>% 
    filter(date >= as.Date("2007-06-22") & date <= as.Date("2009-06-23"))

first_gfc <- gfc_df %>% slice(1) %>% pull(Rho)
last_gfc <- gfc_df %>% slice(n()) %>% pull(Rho)
max_gfc <- max(gfc_df$Rho)

max_gfc - first_gfc
last_gfc - first_gfc


## Covid-19 calculations
covid_df <- dcc.time.var.cor %>% 
    filter(Pairs == "JSE_SP") %>% 
    filter(date >= as.Date("2020-02-15") & date <= as.Date("2021-12-01"))

first_covid <- covid_df %>% slice(1) %>% pull(Rho)
last_covid <- covid_df %>% slice(n()) %>% pull(Rho)
max_covid <- max(covid_df$Rho)

max_covid - first_covid
last_covid - first_covid
```


### Go-Garch

Next I fit a Go-Garch model. This model is based on the same gjrGARCH specification as the DCC model.

```{r, warning=FALSE}
#Spec GO-GARCH
spec.go <- gogarchspec(multi_univ_garch_spec, 
                       distribution.model = 'mvnorm', # or manig.
                       ica = 'fastica') # Note: we use the fastICA
cl <- makePSOCKcluster(10)
multf <- multifit(multi_univ_garch_spec, garch_xts, cluster = cl)

#Fit GO-GARCH
fit.gogarch <- gogarchfit(spec.go, 
                      data = garch_xts, 
                      solver = 'hybrid', 
                      cluster = cl, 
                      gfun = 'tanh', 
                      maxiter1 = 40000, 
                      epsilon = 1e-08, 
                      rseed = 100)

print(fit.gogarch)

# Extracting time-varying conditional correlations
gog.time.var.cor <- rcor(fit.gogarch)
gog.time.var.cor <- aperm(gog.time.var.cor,c(3,2,1))
dim(gog.time.var.cor) <- c(nrow(gog.time.var.cor), ncol(gog.time.var.cor)^2)
# Rename TV correlations
gog.time.var.cor <-
renamingdcc(ReturnSeries = garch_xts, DCC.TV.Cor = gog.time.var.cor)

#Plot correlations for JSE
gog_JSE_gjr <- ggplot(gog.time.var.cor %>% dplyr::filter(grepl("JSE_", Pairs ), 
                                                         !grepl("_JSE", Pairs))) + 
    geom_line(aes(x = date, y = Rho, colour = Pairs), linewidth = 1) + 
    
    annotate("rect", xmin = as.Date("2007-06-22"), xmax = as.Date("2009-06-23"),
    ymin = -Inf, ymax = Inf, fill = "red", alpha = 0.4)+
    annotate("rect", xmin = as.Date("2020-03-15"), xmax = as.Date("2022-06-20"),
    ymin = -Inf, ymax = Inf, fill = "blue", alpha = 0.4)+
    
    theme_fmx()+
    ggtitle("Dynamic Conditional Correlations: JSE")+
    scale_x_date(date_breaks = "4 years", date_labels = "%Y")

finplot(gog_JSE_gjr)

```

The code that follows once again calculates differences during the GFC and Covid-19

```{r}
# This code extracts the correlations for the GFC and COvid-19.
# It then calculates the difference between the first and max, and the first and
# last correlation.

## GFC calculations
gfc_df <- gog.time.var.cor %>% 
    filter(Pairs == "JSE_SP") %>% 
    filter(date >= as.Date("2007-06-22") & date <= as.Date("2009-06-23"))

first_gfc <- gfc_df %>% slice(1) %>% pull(Rho)
last_gfc <- gfc_df %>% slice(n()) %>% pull(Rho)
max_gfc <- max(gfc_df$Rho)

max_gfc - first_gfc
last_gfc - first_gfc


## Covid-19 calculations
covid_df <- gog.time.var.cor %>% 
    filter(Pairs == "JSE_SP") %>% 
    filter(date >= as.Date("2020-02-15") & date <= as.Date("2021-12-01"))

first_covid <- covid_df %>% slice(1) %>% pull(Rho)
last_covid <- covid_df %>% slice(n()) %>% pull(Rho)
max_covid <- max(covid_df$Rho)

max_covid - first_covid
last_covid - first_covid
```


### BEKK-GARCH

Lastly I fit a BEKK-GARCH model to estimate the spillover effects between the 3 return series. 

```{r, warning=FALSE, results='hide'}
#Convert dataframe to matrix as the BEKK function requires a matric
garch_df <- joinedDF %>% 
    select(c(SP500, JSE40, ZARUSD)) %>% 
    rename(SP = SP500,
           JSE = JSE40,
           ZARUSD = ZARUSD) 

garch_matrix <- as.matrix(garch_df)

#Estimate the BEKK-GARCH model
estimated <- mgarchBEKK::BEKK(garch_matrix)



```

The default output of the fitting the BEKK model is parameter matrices. This is quite convenient, however for the sake of the write up a table will work better. I therefore write a simple matrix extractor function that takes each element in a matrix and puts it in a dataframe. I then create such a dataframe for each parameter matrix and bind them by rows.

```{r}
extract_matrix_elements <- function(matrix, prefix) {
  # Initialize an empty dataframe with appropriate column names
  result_df <- data.frame(Position = character(), 
                          Value = numeric(), 
                          stringsAsFactors = FALSE)

  # Loop through each element of the matrix
  for (row in 1:nrow(matrix)) {
    for (col in 1:ncol(matrix)) {
      # Create a position string (e.g., "(2,1)")
        #The prefix arguments serves as a way to identify which matrix 
        #the coefficient is from
      position <- paste(prefix ,"(", row, ",", col, ")", sep = "") 
      
      # Append the position and value to the dataframe
      result_df <- rbind(result_df, data.frame(Position = position, 
                                               Value = matrix[row, col]))
    }
  }
  
  return(result_df)
}

#Extract constants
garch_constants <- extract_matrix_elements(estimated$est.params[[1]], "C")
garch_constants_se <- extract_matrix_elements(estimated$asy.se.coef[[1]], "C")

garch_result_df <- left_join(garch_constants, garch_constants_se, by="Position") %>% 
    filter(Value.x != 0) #Filter out zeros, since the matrix is triangular

#Extract ARCH effects
garch_amat <- extract_matrix_elements(estimated$est.params[[2]], "A")
garch_amat_se <- extract_matrix_elements(estimated$asy.se.coef[[2]], "A")

amat_joined <- left_join(garch_amat, garch_amat_se, by="Position")

#Extract GARCH effects
garch_bmat <- extract_matrix_elements(estimated$est.params[[3]], "B")
garch_bmat_se <- extract_matrix_elements(estimated$asy.se.coef[[3]], "B")

bmat_joined <- left_join(garch_bmat, garch_bmat_se, by="Position")

garch_result_df <- rbind(garch_result_df, amat_joined)
garch_result_df <- rbind(garch_result_df, bmat_joined)
colnames(garch_result_df) <- c(" ", "Coefficient", "Std. Error")

# Add stars indicating significant levels
garch_results_final <- garch_result_df %>% 
    mutate(TStat = Coefficient / `Std. Error`,
           Significance = ifelse(abs(TStat) > 2.576, "***", 
                                 ifelse(abs(TStat) > 1.96, "**", 
                                        ifelse(abs(TStat) > 1.645, "*", ""))))

#Output entire table
kable(garch_results_final, caption = "BEKK-GARCH results")
```

What you are left with is a table with all the parameters in the BEKK-GARCH model and their significance levels. The output for the significance levels appear strange here but work fine within the formal document.

```{r, results='asis'}
## You can uncomment this code if you prefer the results to be represented in
##  matrices

# BEKK_estimates <- estimated$est.params
# names(BEKK_estimates) <- c("Constants", "ARCH estimates", "GARCH estimates")
# 
# BEKK_se <- estimated$asy.se.coef
# names(BEKK_se) <- c("Constants' standard errors", "ARCH standard errors", 
#                     "GARCH standard errors")
# 
# 
# matrix_rename <- function(matrix, headers){
#     colnames(matrix) <- headers
#     rownames(matrix) <- headers
#     return(matrix)
# }
# 
# renamed_BEKK_estimates <- lapply(BEKK_estimates, matrix_rename, 
#                                  headers = c("SP", "JSE", "Rand"))
# renamed_BEKK_se <- lapply(BEKK_se, matrix_rename, 
#                           headers = c("SP", "JSE", "Rand"))
# 
# 
# kable(renamed_BEKK_estimates$Constants,
#       caption = 'Constants')
# kable(renamed_BEKK_se$`Constants' standard errors`,
#       caption = "Constants' standard errors")
# 
# kable(renamed_BEKK_estimates$`ARCH estimates`,
#       caption = "ARCH Estimates")
# kable(renamed_BEKK_se$`ARCH standard errors`,
#       caption = "ARCH standard errors")
# 
# kable(renamed_BEKK_estimates$`GARCH estimates`,
#       caption = "GARCH Estimates")
# kable(renamed_BEKK_se$`GARCH standard errors`,
#       caption = "GARCH standard errors")



```
